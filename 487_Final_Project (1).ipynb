{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tohmt9SwuMA",
        "outputId": "e693cff0-ba4d-4ace-8f68-a79ce25ddcf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/notshrirang/spotify-million-song-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.7M/20.7M [00:00<00:00, 68.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/notshrirang/spotify-million-song-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"notshrirang/spotify-million-song-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-process**"
      ],
      "metadata": {
        "id": "sC4NxQf7brmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the dataset CSV file\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/notshrirang/spotify-million-song-dataset/versions/1/spotify_millsongdata.csv\"\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(dataset_path)\n",
        "df = df[[\"artist\", \"text\"]]\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Check column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1K_ffZGyB8j",
        "outputId": "f9e0db1d-7b4c-4789-eb61-54e7f5d7dc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  artist                                               text\n",
            "0   ABBA  Look at her face, it's a wonderful face  \\r\\nA...\n",
            "1   ABBA  Take it easy with me, please  \\r\\nTouch me gen...\n",
            "2   ABBA  I'll never know why I had to go  \\r\\nWhy I had...\n",
            "3   ABBA  Making somebody happy is a question of give an...\n",
            "4   ABBA  Making somebody happy is a question of give an...\n",
            "Index(['artist', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_artists = len(set(df['artist']))\n",
        "print(num_artists)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CO2RVFvbhY8",
        "outputId": "de192f84-4681-4522-d900-53577bd178d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordDataset and RNN**"
      ],
      "metadata": {
        "id": "AAI2mA2Ib3wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "  def __init__(self, df, artist, seq_len=100, device='cpu'):\n",
        "    self.seq_len = seq_len\n",
        "    self.df = df[df[\"artist\"] == artist]\n",
        "    self.text = self.df['text'].str.cat(sep=' ')\n",
        "\n",
        "    # Tokenize text while preserving newlines (\\r\\n)\n",
        "    self.tokens = re.findall(r\"\\w+(?:'\\w+)?|[\\r\\n]+|[.,!?;:]\", self.text)\n",
        "    self.vocab = sorted(set(self.tokens))\n",
        "\n",
        "    self.wordtoidx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "    self.idxtoword = {idx: word for idx, word in enumerate(self.vocab)}\n",
        "    self.device = device\n",
        "    self.encoded = [self.wordtoidx[word] for word in self.tokens if word in self.wordtoidx]\n",
        "\n",
        "  def __len__(self):\n",
        "    # replace this with code to return the number of possible sub-sequences\n",
        "    return len(self.encoded) - self.seq_len\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    # Get the sequence of token indices and the target (next token in sequence)\n",
        "    X = self.encoded[i:i+self.seq_len]  # Input sequence\n",
        "    y = self.encoded[i+1:i+self.seq_len+1]  # Target sequence (shifted by 1)\n",
        "    # Return both as tensors, where X is the input and y is the target\n",
        "    return torch.tensor(X, device=self.device), torch.tensor(y, device=self.device)\n",
        "\n",
        "  def decode(self,tokens):\n",
        "    # replace this with code to convert a sequence of tokens back into a string\n",
        "    words = [self.idxtoword[token] for token in tokens]\n",
        "    text = \"\"\n",
        "    for word in words:\n",
        "      if word == \"\\r\\n\":\n",
        "        text += word\n",
        "      elif word in \".,!?;:\":\n",
        "        text = text.rstrip() + word\n",
        "      else:\n",
        "        text += \" \" + word\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "ItJ8dl9k46HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed_sheeran_dataset = WordDataset(df, \"Ed Sheeran\", seq_len=75)\n",
        "vocab = ed_sheeran_dataset.vocab\n",
        "\n",
        "print(vocab)\n",
        "print(ed_sheeran_dataset.decode(ed_sheeran_dataset.encoded[:500]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvQW3ZgHgIqO",
        "outputId": "9e046413-077d-4d6a-bc45-7c69428a8772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\r\\n', '\\r\\n\\r\\n', '!', ',', '.', '1', '10', '2', '22', '22nd', '23', '2x', '3', '3008', '31', '70', ':', '?', 'A', 'Act', 'After', 'Against', 'Ah', 'Alas', 'Albert', 'All', 'Almost', 'Although', 'America', 'Amsterdam', 'An', 'Anchored', 'And', 'Angels', 'Another', 'Are', 'Aretha', 'As', 'Atha', 'Awakening', 'Baby', 'Back', 'Be', 'Beat', 'Because', 'Been', 'Before', 'Beside', 'Better', 'Billy', 'Black', 'Blood', 'Blue', 'Both', 'Breakin', 'Breathing', 'Bridge', 'Brighton', 'Bring', 'Broken', 'Burning', 'Burnt', 'But', 'By', 'Call', 'Calling', 'Came', 'Can', \"Can't\", 'Cardiac', 'Cars', 'Catfish', 'Cause', 'Cellophane', 'Celtics', 'Chasing', 'Chevrolet', 'Child', 'Chorus', 'Close', 'Closed', 'Closure', 'Coke', 'Cold', 'Come', 'Committed', 'Confer', 'Confined', 'Constantly', 'Constrict', 'Cos', 'Could', 'Couple', 'Cover', 'Covered', 'Creating', 'Crumbling', 'Cuddle', \"D's\", 'Da', 'Daddy', 'Darling', 'December', 'Desolation', \"Didn't\", 'Differently', 'Dirty', 'Disappearing', 'Distance', 'Dixie', 'Do', 'Dodging', \"Don't\", 'Done', 'Down', 'Drinking', 'Dry', \"Durin's\", 'Easy', 'Echoed', 'Ed', 'Either', 'End', \"Estate's\", 'Even', 'Everybody', 'Everything', \"Everything's\", 'Except', 'Face', 'Faces', 'Fact', 'Fading', 'Falling', 'Feel', 'Feeling', 'Fighting', 'First', 'Flames', 'Float', 'For', 'Forget', 'Forward', 'Found', 'Four', 'Fresh', 'Friends', 'From', 'Fuck', 'Fulfills', 'Getting', 'Girl', 'Give', 'Glass', 'Go', 'God', \"God's\", 'Going', 'Gonna', 'Good', 'Goodbye', 'Gotta', 'Had', 'Hallelujah', 'Hanging', 'Harley', 'Have', 'Hear', 'Hearing', 'Heart', 'Hell', 'Hey', 'Highbury', 'Hold', 'Holding', 'Hollowing', 'Hollywood', 'Home', 'Hoping', 'Hot', 'How', \"How'd\", 'Hunger', 'Hush', 'I', \"I'd\", \"I'll\", \"I'm\", \"I've\", 'If', 'Ignoring', 'In', 'Inside', 'Instead', 'Into', 'Intro', 'Is', 'It', \"It's\", 'Its', 'Jack', 'Jean', 'Jordan', 'Just', 'K', 'Keep', 'Kill', 'Kiss', 'Knock', 'Know', 'La', 'Lace', 'Late', 'Lately', 'Lay', 'Layed', 'Led', 'Left', 'Lego', 'Let', 'Lie', 'Lights', 'Like', 'Listen', 'Little', 'London', 'Loneliness', 'Long', 'Look', 'Loose', 'Lord', 'Lost', 'Louder', 'Love', 'Loving', 'Lurk', 'M', 'Mac', 'Make', 'Marilyn', 'Maybe', 'Meaning', 'Michael', 'Miles', 'Mistakes', 'Mmm', 'Monroe', 'More', 'Morpheus', 'Music', 'My', 'N', 'Never', 'New', 'Next', 'Nina', 'No', 'Nonchalant', 'None', 'Norma', 'Not', 'Nothing', 'November', 'Now', 'Of', 'Off', 'Oh', 'Okay', 'Old', 'On', 'One', 'Only', 'Ooh', 'Oooh', 'Open', 'Or', 'Our', 'Outro', 'Outside', 'Overlook', 'Paid', 'Pain', 'Paint', 'Passes', 'Past', 'Patience', 'Pedal', 'People', 'Place', 'Planet', 'Played', 'Please', 'Portellies', 'Pre', 'Prepare', 'Put', 'Raise', 'Really', 'Reds', 'Remove', 'Replace', 'Ride', 'Right', 'Ripped', 'Ripping', 'Rise', 'Road', 'Rolling', 'Rosalie', 'Roxy', 'Running', 'Sadness', 'Schoolbooks', 'Searching', 'See', 'Send', 'Sends', 'Settle', 'Sew', 'She', \"She'll\", \"She's\", 'Sheeran', 'Sheriff', 'Shock', 'Shooting', 'Should', 'Sing', 'Singin', 'Singing', 'Sipping', 'Sirens', 'Sittin', 'Six', 'Sixth', 'Sleep', 'Slowly', 'Smoke', 'Smoking', 'So', 'Someone', \"Something's\", 'Sometimes', 'South', 'Speaker', 'Stapled', 'Stay', 'Stevie', 'Stick', 'Stops', 'Straight', 'Stranger', 'Streets', 'Struggling', 'Stuck', 'Suffocating', 'Surely', 'Sweet', 'Swift', 'Take', 'Taking', 'Taylor', 'Teach', 'Tears', 'Tell', 'Tempo', 'Than', 'That', \"That's\", 'The', 'Then', 'There', \"There'd\", \"There's\", 'Therefore', 'These', 'They', \"They'll\", \"They're\", 'Things', 'This', 'Those', 'Though', 'Thought', 'Til', 'Till', 'To', 'Told', 'Tonight', 'Touch', 'Traveling', 'Treat', 'Tried', 'Trust', 'Turned', 'U', 'UK', 'Uh', 'Unconditional', 'Under', 'Underneath', 'Unhappy', 'Uni', 'Until', 'Verse', 'Voices', 'W', 'Wait', 'Waiting', 'Waking', 'Wanted', 'Warm', 'Was', 'Watch', 'Watched', 'Watching', 'Wayne', 'We', \"We'd\", \"We'll\", \"We're\", 'Weary', 'Weeks', 'Weigh', 'Well', 'What', \"What's\", 'Whats', 'When', 'Whenever', 'Where', 'While', 'White', 'Who', 'Whoaahh', 'Will', 'Wish', 'With', 'Without', 'Woah', 'Woahhh', 'Woke', \"Won't\", 'Wonder', 'Would', 'Yeah', 'Yesterday', 'York', 'You', \"You'd\", \"You'll\", \"You're\", 'Your', 'a', 'about', 'above', 'aching', 'across', 'act', 'afire', 'afloat', 'afraid', 'after', 'again', 'against', 'ages', 'ahead', 'aim', \"ain't\", 'aint', 'air', \"airport's\", 'alarm', 'alarms', 'alcohol', 'alight', 'alive', 'all', 'allow', 'almost', 'alone', 'along', 'already', 'alright', 'also', 'always', 'am', 'amongst', 'an', 'and', 'angel', 'angels', 'another', 'any', 'anybody', 'anymore', 'anything', 'anyway', 'anywhere', 'apart', 'applaud', 'applause', 'are', \"aren't\", 'armband', 'arms', 'around', 'arrive', 'as', 'ash', 'asked', 'asleep', 'at', 'attack', 'attention', 'auburn', 'auto', 'automatic', 'autumn', 'awake', 'away', 'awkward', 'babe', 'babies', 'baby', 'bachelor', 'back', 'backpack', 'backseat', 'bag', 'bailey', 'baked', 'ballpoint', \"band's\", 'bank', 'bankers', 'barely', 'bars', 'base', 'based', 'be', 'beat', 'beating', 'beats', 'beautiful', 'because', 'bed', 'bedroom', 'beds', 'been', 'beer', 'before', 'began', 'beginning', 'behind', 'bein', 'being', 'believe', 'believed', 'bell', 'belly', \"belly's\", 'belong', 'below', 'beneath', 'beside', 'best', 'better', 'between', 'beyond', 'bible', 'big', 'bike', 'bill', 'bills', 'bird', 'birds', 'bit', 'bitter', 'bitterness', 'black', 'blaring', 'blaze', 'blazing', 'bled', 'bleed', 'bleeding', 'blind', 'blink', 'block', 'blocked', 'blocking', 'blood', 'bloodstream', 'blows', 'blunt', 'blur', 'boat', 'body', 'bones', 'boost', 'born', 'both', 'bottle', 'bottled', 'bottles', 'bottom', 'bout', 'boy', 'boys', 'braces', 'brain', 'break', 'breaking', 'breath', 'breathe', 'breathing', 'breathless', 'breeze', 'bridge', 'bright', 'brighten', 'bring', 'brings', 'broke', 'broken', 'brother', 'brothers', 'bruises', 'bubble', 'build', 'bulletproof', 'burgers', 'burgh', 'burn', 'burned', 'burning', 'burns', 'bus', 'business', 'but', 'butt', 'butterflies', 'by', 'bye', 'cab', 'call', 'called', 'callin', 'calling', 'calls', 'came', 'can', \"can't\", 'candle', 'cap', 'capitalist', 'captain', 'car', 'cards', 'care', 'careful', 'carried', 'carry', 'cars', 'catch', 'cause', 'caused', 'certain', 'champagne', 'chance', 'change', 'changed', 'changes', 'chaos', 'charge', 'chase', 'check', 'cheek', 'cheeks', 'cheese', 'chemicals', 'chest', 'chicago', 'child', \"child's\", 'children', 'chills', 'chosen', 'chunk', 'church', 'cigarette', 'cities', 'city', 'class', 'classes', 'classmates', 'classroom', 'clear', 'clearer', 'climb', 'cling', 'clip', 'clock', 'close', 'closin', 'closing', 'clothes', 'clouds', 'coffee', 'coke', 'cold', 'colder', 'collide', 'color', 'coloured', 'come', 'comes', 'coming', 'commitment', 'company', 'compatible', 'complete', 'complicated', 'complications', 'comrades', 'concept', 'condoms', 'confused', 'control', 'controller', 'cook', 'cookie', 'cool', 'corner', 'coroner', 'cos', 'couch', 'could', \"could've\", \"couldn't\", 'count', 'country', 'couple', 'course', 'court', 'covered', 'covering', 'covers', 'crash', 'crashed', 'craving', 'crawl', 'crawled', 'crazy', 'create', 'created', 'cried', 'crimson', \"critic's\", 'criticism', 'crooked', 'crossed', 'crowbars', 'crowds', 'cruel', 'crumbles', 'crushing', 'cry', 'crying', 'cuffs', 'culture', 'curve', 'curved', 'cut', 'cuz', \"d'you\", 'da', 'dad', 'daddy', 'daily', 'dam', 'dance', 'dances', 'danger', 'dare', 'dark', 'darkness', 'darling', 'date', 'dawn', 'day', 'daydream', 'days', 'dead', 'deadbeat', 'deal', 'dear', 'death', \"death's\", 'dedications', 'deep', 'depart', 'designed', 'destruction', 'devil', 'devotion', 'dew', 'dewlow', 'dice', 'did', \"didn't\", 'die', 'died', 'difference', 'different', 'differently', 'dinner', 'dirt', 'disappear', 'disappeared', 'dissapear', 'distance', 'dive', 'diving', 'do', 'doctors', 'dodging', 'does', \"doesn't\", 'dog', 'doing', \"don't\", 'done', 'door', 'dot', 'double', 'doubt', 'down', 'dragon', 'draw', 'drawer', 'dream', 'dreamers', 'dreams', 'dress', 'drink', 'drinking', 'drive', 'driver', 'driveway', 'driving', 'drop', 'drown', 'drowning', 'drunk', 'dry', 'dude', 'dues', 'dumps', 'duvet', 'dvd', 'dying', \"e'er\", 'each', 'ears', 'earth', 'ease', 'easier', 'eat', 'eating', 'edge', 'eight', 'eighteen', 'either', 'elephant', 'else', 'em', 'empty', 'end', 'ended', 'endless', 'ends', 'enemy', 'enough', 'escape', 'etched', 'evaluating', 'even', 'evening', 'ever', 'evergreen', 'every', 'everybody', 'everyday', 'everything', \"everything's\", 'evidence', 'evil', 'except', 'excuse', 'exit', 'expect', 'ey', 'eye', 'eyed', 'eyes', \"eyes'll\", 'face', 'fact', 'fade', 'faded', 'fades', 'faith', 'fake', 'fall', 'fallin', 'falling', 'falls', 'fam', 'family', 'fans', 'far', 'fast', 'father', 'fault', 'feature', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'fell', 'felt', 'fence', 'fields', 'fight', 'figure', 'figures', 'fill', 'filled', 'filler', 'fills', 'film', 'find', 'finds', 'fine', 'fingers', 'fire', 'firebrigade', 'firefly', 'fires', 'first', 'fit', 'fits', 'five', 'fizzle', 'flag', 'flame', 'flames', 'flashing', 'flat', 'flicker', 'flicking', 'flies', 'flight', 'floating', 'flood', 'flooded', 'floor', 'flossin', 'flowin', 'flows', 'fly', 'folder', 'follower', 'food', 'fool', 'fooling', 'foolish', 'for', 'forearms', 'foreigner', 'forever', 'forget', 'forgetting', 'forgive', 'fork', 'fortune', 'forward', 'foster', 'fought', 'found', 'four', 'frame', 'free', 'freezing', 'friend', 'friends', 'frightened', 'from', 'front', 'froze', 'frozen', 'frustrated', 'fuck', 'fuckin', 'fucking', 'full', 'fulla', 'fun', 'further', 'future', 'gamble', 'games', 'gates', 'gather', 'gave', \"gcse's\", 'gently', 'get', 'gets', 'getting', 'ghost', 'gin', 'girl', 'give', 'gives', 'giving', 'glass', 'global', 'glory', 'gloves', 'glue', 'glued', 'go', 'goals', 'god', 'goddamn', 'goes', 'goin', 'going', 'gold', 'golden', 'gone', 'gonna', 'good', 'goodness', 'got', 'gotta', 'government', 'grab', 'grace', 'grade', 'grades', 'grams', 'grandeur', 'grandma', 'grandson', 'grass', 'great', 'green', 'grew', 'groomed', 'ground', 'group', 'grow', 'grown', 'grows', 'guess', 'guide', 'guitar', 'gun', 'gust', 'guys', 'habits', 'had', \"hair's\", 'hairband', 'halellujah', 'half', 'hallelujah', 'halls', 'hand', 'handed', 'hands', 'happened', 'happy', 'hard', 'harder', 'harm', 'has', 'hate', 'hated', 'have', \"haven't\", 'hawk', 'he', \"he's\", 'head', \"head's\", 'heal', 'hear', 'heard', 'heart', \"heart's\", 'heartbeat', 'hearted', 'hearts', 'heat', 'heating', 'heaven', 'heavy', 'hell', \"hell's\", 'help', 'helps', 'her', 'here', 'hers', 'hey', 'hide', 'hiding', 'high', 'him', 'his', 'hit', 'hitting', 'hold', 'holding', 'home', 'homeless', 'homework', 'homie', 'honest', 'honey', 'honor', 'honour', 'hood', 'hoodie', 'hope', 'hoped', 'hopes', 'hoping', 'horizons', 'horny', 'horror', 'hotel', 'hounded', 'hours', 'house', 'how', \"how'd\", 'hurt', 'hurts', 'husband', 'hype', \"i'd\", 'iPod', 'ice', 'idiot', 'if', 'ignore', 'illegal', 'image', 'in', 'ink', 'inside', 'insides', 'inspired', 'intended', 'intention', 'intertwined', 'into', 'involved', 'is', \"isn't\", 'it', \"it's\", 'its', 'jam', 'jeans', 'job', 'jockey', 'joke', 'joy', 'judge', 'juices', 'jump', 'just', 'keep', 'keeper', 'keeps', 'kept', 'kicks', 'kid', 'kill', 'killer', 'kinda', 'kindness', 'kings', 'kiss', 'kissed', 'kissing', 'kitchen', 'kites', 'knapsack', 'knew', 'knock', 'know', 'knowing', 'known', 'knows', 'la', 'lace', 'lack', 'lady', 'lahmlahlah', 'lamppost', 'land', 'landscape', 'lane', 'last', 'lasts', 'late', 'lately', 'later', 'laugh', 'law', 'lay', 'laying', 'lead', 'leads', 'lean', 'learn', 'least', 'leave', 'leaves', 'leaving', 'led', 'left', 'leg', 'legend', 'legs', 'lemon', 'less', 'lesson', 'let', \"let's\", 'letting', 'liar', 'lie', 'lied', 'life', \"life's\", 'light', 'lighten', 'lightning', 'lights', 'like', 'liked', 'limit', 'limits', 'line', \"line's\", 'lines', 'lion', 'lip', 'lips', 'liquor', 'listen', 'listening', 'lit', 'little', 'live', 'lived', 'liver', 'lives', 'living', 'local', 'lone', 'lonely', 'long', 'longer', 'longest', 'look', 'looked', 'lookin', 'looking', 'looks', 'loose', 'lord', 'lose', 'lost', 'lot', 'lots', 'loud', 'love', \"love's\", 'loved', 'lover', 'loves', 'loving', \"loving's\", 'low', 'lullaby', 'lump', 'lumps', 'lungs', 'lust', 'lying', 'm', 'mad', 'made', 'make', 'makes', 'making', 'mama', 'man', \"man's\", 'many', 'matchstick', 'mate', 'matter', 'may', 'maybe', 'me', 'meals', 'mean', 'meanest', 'meaningless', 'meanings', 'meant', 'medicine', 'meet', 'melody', 'melt', 'memories', 'memory', 'men', 'mend', 'mending', 'mentally', 'mercy', 'mess', 'message', 'messed', 'met', 'mic', 'mics', 'midnight', 'might', 'miles', 'mind', \"mind's\", 'minds', 'mine', 'minute', 'mirror', 'mirrors', 'miss', 'missing', 'mistake', 'mistakes', 'misty', 'mixing', 'mm', 'mmm', 'moan', 'mom', 'moment', 'money', 'months', 'moon', 'more', 'morning', 'mossy', 'most', 'mother', 'motherland', 'motions', 'mountain', 'mouth', 'move', 'moved', 'movements', 'moves', 'much', 'mum', 'murdered', 'muscle', 'music', 'musical', 'must', \"mustn't\", 'my', 'myself', 'mysterious', 'name', 'nan', 'nasty', 'nature', 'near', 'neck', 'necklace', 'need', 'needed', 'needle', 'needs', 'neon', 'nervous', 'never', 'new', 'next', 'nice', 'night', 'nights', 'no', 'noise', 'non', 'none', 'nor', 'not', 'note', 'notes', 'nothing', 'now', 'nowhere', 'nude', 'numb', 'number', 'numbers', 'nurse', 'oak', 'ocean', 'oceans', 'of', 'off', 'oh', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'ooh', 'open', 'or', 'other', \"other's\", 'ought', 'our', 'ourselves', 'out', 'outside', 'over', 'overrated', 'owe', 'own', 'oxygen', 'pack', 'packed', 'pages', 'paid', 'pain', 'painful', 'paint', 'painting', 'pale', 'papers', 'paperwork', 'parents', 'part', 'parting', 'partner', 'party', 'pass', 'passed', 'past', 'pastries', 'path', 'paths', 'pathway', 'patient', 'pause', 'pavement', 'pay', 'peace', 'pee', 'peepin', 'pen', 'pennies', 'people', 'perfect', 'perhaps', 'persuaded', 'phone', 'photograph', 'phrases', 'pick', 'pickin', 'picture', 'piece', 'pieces', 'pier', 'pill', 'pillow', 'pilot', 'pipe', 'pizza', 'place', \"place'll\", 'placed', 'plan', 'plane', 'planes', 'plans', 'play', 'played', 'playin', 'playing', 'plays', 'please', 'plot', 'pocket', 'podium', 'pole', 'pony', 'poor', 'portion', 'position', 'pot', 'power', 'pray', 'prayer', 'prefers', 'press', 'pressed', 'pretend', 'pretends', 'pretty', 'price', 'profound', 'program', 'promise', 'proof', 'proud', 'prove', 'pulled', 'pump', 'purest', 'purpose', 'put', 'puts', 'quad', 'quarter', 'questioning', 'quick', 'quiet', 'quite', 'rabbits', 'racing', 'radio', 'rage', 'raging', 'rain', 'raincoat', 'raised', 'rate', 'rather', 're', 'reach', 'read', 'ready', 'real', 'realize', 'really', 'reason', 'recall', 'reckon', 'recognise', 'red', 'refine', 'regardless', 'regret', 'relative', 'relevant', 'reload', 'rely', 'remember', 'remind', 'rent', 'replace', 'requires', 'resembling', 'respect', 'rest', 'resting', 'returned', 'returns', 'riches', 'ride', 'rides', 'right', 'ringing', 'ripped', 'rise', 'river', 'road', 'roads', 'roadway', 'roam', 'rock', 'role', 'roll', 'rolled', 'rollin', 'roof', 'room', 'roots', 'rot', 'rough', 'round', 'row', 'run', 'runaway', 'running', 'rush', 'rushing', 'rusted', 'sack', 'sad', \"saddle's\", 'safe', 'safety', 'said', 'sailor', 'saints', 'sake', 'salt', 'salted', 'same', 'sand', 'sang', 'sat', 'save', 'saving', 'savior', 'saw', 'say', 'saying', 'says', 'scales', 'scar', 'scarred', 'scars', 'scene', 'school', 'schools', 'scream', 'screamin', 'screaming', 'searched', 'searching', 'seas', 'season', 'seats', 'secrets', 'sedative', 'see', 'seed', 'seein', 'seeing', 'seek', 'seem', 'seemed', 'seems', 'seen', 'sees', 'self', 'selfish', 'sell', 'sells', 'sensation', 'senses', 'serving', 'sessions', 'set', 'settle', 'seven', 'sew', 'sex', 'sexual', 'shadow', 'shake', 'shall', 'shape', 'she', \"she'll\", \"she's\", 'sheets', 'shelter', 'sheltered', 'shine', 'shirt', 'shirtsleeve', 'shirtsleeves', 'shiver', 'shoelaces', 'shoes', 'shooting', 'shop', 'shopping', 'shore', 'short', 'shotgun', 'shots', 'should', \"should've\", 'shoulder', 'show', 'shower', 'shows', 'shunned', 'shut', 'shuts', 'sick', 'sickness', 'side', 'sigh', 'sight', 'sign', 'signed', 'signs', 'silence', 'silent', 'simple', 'since', 'sing', 'singing', 'single', 'sinking', 'sinning', 'sins', 'sisters', 'sit', 'sitting', 'situation', 'six', 'sixteen', 'skin', 'sky', 'skyline', 'slain', 'sleep', 'sleeps', 'sleeve', 'slept', 'slightly', 'slow', 'slowly', 'small', 'smart', 'smile', \"smile's\", 'smoke', 'smoking', 'smooth', 'snaps', 'snowflakes', 'so', 'sober', 'socks', 'softly', 'some', 'somehow', 'something', 'sometimes', 'somewhere', 'son', 'sonar', 'song', 'songs', 'soon', 'sorrows', 'sorry', 'soul', 'souls', 'sound', 'sour', 'spark', 'speak', 'speakers', 'spent', 'spill', 'spine', 'spinning', 'splattered', 'splits', 'spoke', 'spoon', 'spot', 'spring', 'squeeze', 'squeezed', 'stacks', 'stage', 'stalker', 'stalling', 'stand', 'standin', 'stands', 'star', 'stare', 'staring', 'stars', 'start', 'started', 'starts', 'stay', 'stayed', 'staying', 'stealing', 'steelo', 'steep', 'step', 'sticks', 'still', 'stomach', 'stone', 'stop', 'stopped', 'stops', 'storing', 'storm', 'story', 'straight', 'strange', 'stranger', 'strangers', 'strawberries', 'stray', 'stream', 'street', 'streets', 'stride', 'strings', 'strong', 'stronger', 'struck', 'strumming', 'stubborn', 'stumbling', 'stung', 'such', 'sugar', 'suicide', 'suit', 'suitcase', 'suited', 'summer', 'sun', 'sunburn', 'sunny', 'sunset', 'superstar', 'supposed', 'suppress', 'sure', 'surely', 'surface', 'surrender', 'swap', 'swear', 'sweep', 'sweet', 'sweethearts', 'swim', 'swing', 'switch', 'symphony', 'tail', 'take', 'taked', 'taken', 'takes', 'taking', 'talents', 'talk', 'talked', 'talking', 'tap', 'taste', 'tattoo', 'tattooed', 'taught', 'tea', 'teach', 'teacher', 'teachers', 'team', 'tear', 'teardrops', 'tears', 'teen', 'teeth', 'tell', 'tells', 'tequila', 'test', 'text', 'than', 'that', \"that's\", 'the', 'their', 'them', 'then', 'there', \"there's\", 'these', 'they', \"they'll\", \"they're\", 'thigh', 'thighs', 'thing', 'things', 'think', 'thinking', 'this', \"this'll\", 'those', 'though', 'thought', 'thoughts', 'thousand', 'threat', 'three', 'throat', 'through', 'throw', 'thrown', 'thunderous', 'thus', 'tides', 'tie', 'tight', 'til', 'till', 'time', \"time's\", 'times', 'tired', 'tireless', 'to', 'today', 'toe', 'together', 'toil', 'token', 'told', 'tomorrow', 'ton', 'tone', 'tongue', \"tongue's\", 'tonight', 'too', 'took', 'torn', 'touch', 'tough', 'toughest', 'tour', 'tow', 'tower', 'town', 'track', 'trades', 'traffic', 'train', 'trance', 'traveler', 'tray', 'treadmill', 'treat', 'tree', 'trees', 'trespassing', 'tricks', 'tried', 'trigonometry', 'trips', 'trouble', 'true', 'trust', 'truth', 'try', 'tsunami', 'tumbles', 'turn', 'turned', 'turns', 'twenty', 'two', 'type', 'uh', 'unaware', 'under', 'underneath', 'underpaid', 'understand', 'understands', 'underway', 'untied', 'until', 'unto', 'up', 'upon', 'upper', 'ups', 'us', 'use', 'used', 'uses', 'usually', 'vain', 'vanish', 'veins', 'verge', 'very', 'vibe', 'views', 'vivid', 'voice', 'voices', 'wages', 'wait', 'waitin', 'waiting', 'wake', 'wakes', 'wakin', 'waking', 'walk', 'walked', 'walking', 'wall', 'walls', 'wanna', 'want', 'wanted', 'wanting', 'wants', 'war', 'warm', 'warning', 'warrior', 'was', \"wasn't\", 'waste', 'wasting', 'watch', 'watched', 'watching', 'water', 'waterside', 'waves', 'waving', 'way', 'wayfaring', 'ways', 'we', \"we'll\", \"we're\", \"we've\", 'wearing', 'weary', 'weed', 'week', 'weekend', 'weeks', 'weighs', 'weight', 'welcomed', 'well', 'went', 'were', \"weren't\", 'wet', 'what', \"what's\", 'when', 'where', 'which', 'while', 'whiskey', 'whisky', 'whisper', 'whispered', 'white', 'who', \"who'd\", 'whoa', 'why', 'wide', 'width', 'wife', 'wild', 'will', 'wind', 'windowsill', 'wine', 'wings', 'winter', 'wipe', 'wiped', 'wish', 'wishing', 'wit', 'with', 'within', 'without', 'woah', 'woke', 'women', \"won't\", 'wonder', 'wondered', 'woo', 'woodwork', 'wooo', 'words', 'work', 'world', 'worry', 'worse', 'worst', 'worth', 'would', \"wouldn't\", 'wound', 'wow', 'wrapped', 'write', 'writing', 'written', 'wrong', 'wrote', 'x19', 'x2', 'x4', 'x5', 'x6', 'x7', 'ya', 'yard', 'yeah', 'year', 'years', 'yes', 'yesterday', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'young', 'your', 'yours', 'yourself', 'zone']\n",
            "It's alright to cry even my dad does sometimes\r\n",
            " So don't wipe your eyes\r\n",
            " Tears remind you you're alive\r\n",
            " It's alright to die cause death the only thing you haven't tried\r\n",
            " But just for tonight hold on\r\n",
            " So live life like you're giving all\r\n",
            " Cause you act like you are\r\n",
            " Go ahead and just live it up\r\n",
            " Go on and tell me your path\r\n",
            "\r\n",
            " It's alright to shake\r\n",
            " Even my hand does sometimes\r\n",
            " So inside the rage Against the dying of the light\r\n",
            " It's alright to say that death's\r\n",
            " The only thing you haven't tried\r\n",
            " But just for today hold on\r\n",
            " So live life like you're giving all\r\n",
            " Cause you act like you are\r\n",
            " Go ahead and just live it up\r\n",
            " Go on and tell me your path\r\n",
            "\r\n",
            " Go ahead and just live it up\r\n",
            " Go on and tell me your path and hold on \r\n",
            "\r\n",
            " We're not, no we're not friends, nor have we ever been\r\n",
            " We just try to keep those secrets in our lives\r\n",
            " And if they find out, will it all go wrong?\r\n",
            " I never know, no one wants it to\r\n",
            "\r\n",
            " So I could take the back road\r\n",
            " But your eyes'll lead me straight back home\r\n",
            " And if you know me like I know you\r\n",
            " You should love me, you should know\r\n",
            "\r\n",
            " Friends just sleep in another bed\r\n",
            " And friends don't treat me like you do\r\n",
            " Well I know that there's a limit to everything\r\n",
            " But my friends won't love me like you\r\n",
            " No, my friends won't love me like you\r\n",
            "\r\n",
            " We're not friends, we could be anything\r\n",
            " If we tried to keep those secrets safe\r\n",
            " No one will find out if it all went wrong\r\n",
            " They'll never know what we've been through\r\n",
            "\r\n",
            " So I could take the back road\r\n",
            " But your eyes'll lead me straight back home\r\n",
            " And if you know me like I know you\r\n",
            " You should love me, you should know\r\n",
            "\r\n",
            " Friends just sleep in another bed\r\n",
            " And friends don't treat me like you do\r\n",
            " Well I know that there's a limit to everything\r\n",
            " But my friends won't love me like you\r\n",
            " No, my friends won't love me like you\r\n",
            "\r\n",
            " But then again, if we're not friends\r\n",
            " Someone else might love you too\r\n",
            " And then again, if we're not friends\r\n",
            " There'd be nothing I could do, and that's why\r\n",
            "\r\n",
            " Friends should sleep in other beds\r\n",
            " And friends should kiss me like you do\r\n",
            " And I know that there's a limit to everything\r\n",
            " But my friends won't love me like you\r\n",
            " No, my friends won't love me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class LyricRNN(nn.Module):\n",
        "  def __init__(self,vocabulary_size,hidden_size=256):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocabulary_size,hidden_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.U = nn.Linear(hidden_size,hidden_size)\n",
        "    self.W = nn.Linear(hidden_size,hidden_size)\n",
        "    self.act = nn.SiLU()\n",
        "    self.V = nn.Linear(hidden_size,vocabulary_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.embedding(x)\n",
        "    B,N = x.shape[:2]\n",
        "    h = torch.zeros(B,self.hidden_size).to(x.device)\n",
        "    Ux = self.U(x)\n",
        "    y = []\n",
        "    for i in range(N):\n",
        "      Wh = self.W(h)\n",
        "      h = self.act(Ux[:,i] + Wh)\n",
        "      y.append(self.V(h))\n",
        "    return torch.stack(y,dim=1)"
      ],
      "metadata": {
        "id": "Lif9gcPC5_iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=300, hidden_size=256, num_layers=3):\n",
        "        super(LyricGRU, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=.3, bidirectional=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, vocab_size)  # Predict next word in vocab\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)  # Shape: (batch, seq_len, embed_size)\n",
        "        x, hidden = self.gru(x, hidden)  # Shape: (batch, seq_len, hidden_size)\n",
        "\n",
        "        # Process each timestep separately, similar to your LyricRNN\n",
        "        y = [self.fc(x[:, i, :]) for i in range(x.shape[1])]\n",
        "\n",
        "        return torch.stack(y, dim=1)  # Shape: (batch, seq_len, vocab_size)\n"
      ],
      "metadata": {
        "id": "xsHjfGOqLTi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=300, hidden_size=256, num_layers=3):\n",
        "        super(LyricLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=.25, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(2*hidden_size)\n",
        "        self.fc = nn.Linear(2*hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: [batch_size, seq_len, embed_size]\n",
        "        lstm_out, _ = self.lstm(x)  # Shape: [batch_size, seq_len, hidden_size]\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "        y = [self.fc(lstm_out[:, i, :]) for i in range(lstm_out.shape[1])]\n",
        "        return torch.stack(y, dim=1)  # Shape: [batch_size, seq_len, vocab_size]\n"
      ],
      "metadata": {
        "id": "hIn7fM3rRBGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model for ABBA**"
      ],
      "metadata": {
        "id": "zoXlam66b0FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train model for just ABBA\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model_type, df, artist, num_epochs=20, batch_size=16, lr=0.001, convergence_threshold=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "  print(f\"Training {model_type} for {artist}..\\n\")\n",
        "  torch.manual_seed(42)\n",
        "  train_dataset = WordDataset(df, artist, seq_len=50, device=device)\n",
        "  train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "  vocab_size = len(train_dataset.vocab)\n",
        "\n",
        "  if model_type == \"RNN\":\n",
        "    model = LyricRNN(vocab_size).to(device)\n",
        "  elif model_type == \"GRU\":\n",
        "    model = LyricGRU(vocab_size).to(device)\n",
        "  elif model_type == \"LSTM\":\n",
        "    model = LyricLSTM(vocab_size).to(device)\n",
        "  else:\n",
        "    raise ValueError(\"Invalid model name. Choose from 'RNN', 'GRU', or 'LSTM'.\")\n",
        "\n",
        "  opt = torch.optim.Adam(model.parameters(),lr)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  model.train()\n",
        "\n",
        "  prev_loss = float('inf')\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch, (X, y) in enumerate(tqdm(train_loader)):\n",
        "      opt.zero_grad()\n",
        "      outputs = model(X)\n",
        "      loss = loss_fn(outputs.view(-1, vocab_size), y.view(-1))\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if abs(prev_loss - avg_loss) < convergence_threshold:\n",
        "      print(f\"Model converged after {epoch+1} epochs.\")\n",
        "      break\n",
        "\n",
        "    prev_loss = avg_loss\n",
        "\n",
        "  return train_dataset, train_loader, model"
      ],
      "metadata": {
        "id": "WYCjeEMNlQ9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(probs, k=5):\n",
        "    \"\"\" Selects from the top-k most probable next words. \"\"\"\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, k)\n",
        "    top_k_probs = torch.softmax(top_k_probs, dim=0)  # Re-normalize probabilities\n",
        "    next_word_idx = top_k_indices[torch.multinomial(top_k_probs, num_samples=1)]\n",
        "    return next_word_idx.item()\n",
        "\n",
        "def top_p_sampling(probs, p=.8):\n",
        "    \"\"\" Nucleus sampling: select from the smallest set of words whose cumulative probability > p. \"\"\"\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
        "    sorted_indices_to_keep = sorted_indices[cumulative_probs <= p]\n",
        "    if len(sorted_indices_to_keep) == 0:  # Ensure we select at least one word\n",
        "        sorted_indices_to_keep = sorted_indices[:1]\n",
        "    sorted_probs = sorted_probs[:len(sorted_indices_to_keep)]\n",
        "    next_word_idx = sorted_indices_to_keep[torch.multinomial(torch.softmax(sorted_probs, dim=0), num_samples=1)]\n",
        "    return next_word_idx.item()\n",
        "\n",
        "def generate_text_stochastic(model, dataset, prompt, num_to_generate=100, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    #print(f'vocab size: {len(dataset.vocab)}')  # Should match model's vocabulary_size\n",
        "    model.eval()\n",
        "    generated_text = prompt.split()\n",
        "    # Get the tokens use encode func in WordDataset\n",
        "    tokens = [dataset.wordtoidx[word] for word in prompt.split() if word in dataset.wordtoidx]\n",
        "\n",
        "    # Generate the text\n",
        "    for _ in range(num_to_generate):\n",
        "        if len(tokens) < dataset.seq_len:\n",
        "            input_seq = torch.tensor(tokens, device=device).unsqueeze(0)\n",
        "        else:\n",
        "            input_seq = torch.tensor(tokens[-dataset.seq_len:], device=device).unsqueeze(0)  # Use last seq_len tokens\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_seq)\n",
        "            #print(f'output shape: {output.shape}')\n",
        "            temperature = 1  # Higher temperature increases randomness\n",
        "            probs = torch.softmax(output[0, -1] / temperature, dim=0)\n",
        "            #next_word_idx = top_p_sampling(probs)\n",
        "            #next_word_idx = top_k_sampling(probs)\n",
        "            next_word_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            #print(f'next word idx: {next_word_idx}')\n",
        "\n",
        "        next_word = dataset.idxtoword[next_word_idx]\n",
        "\n",
        "        if next_word.startswith(\"\\r\\n\"):\n",
        "            generated_text.append(next_word)\n",
        "        elif next_word in \".,!?;:\":\n",
        "            if generated_text:\n",
        "                generated_text[-1] += next_word\n",
        "            else:\n",
        "                generated_text.append(next_word)\n",
        "        else:\n",
        "            generated_text.append(next_word)\n",
        "\n",
        "        tokens.append(next_word_idx) # update tokens\n",
        "\n",
        "    return dataset.decode([dataset.wordtoidx[word] for word in generated_text if word in dataset.wordtoidx])"
      ],
      "metadata": {
        "id": "7zpVexZXv46l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, train_loader, trained_model = train_model(\"LSTM\", df, \"Ed Sheeran\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VHLXpbQEwBt",
        "outputId": "178df5e9-b3bc-4ab7-9d85-6e79e018d1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM for Ed Sheeran..\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:46<00:00, 29.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.2790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:39<00:00, 34.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Loss: 0.0488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:36<00:00, 37.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Loss: 0.0329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:37<00:00, 36.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Loss: 0.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:36<00:00, 36.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Loss: 0.0168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:36<00:00, 36.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Loss: 0.0136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:36<00:00, 37.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Loss: 0.0121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:36<00:00, 37.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Loss: 0.0109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:37<00:00, 36.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Loss: 0.0102\n",
            "Model converged after 9 epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text_stochastic(trained_model, dataset, \"We found love\", num_to_generate=200)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx_XS225FFrX",
        "outputId": "32e176dd-86ce-411c-9966-d259ecddcfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We found love you sky\r\n",
            " To refine the purest of kings\r\n",
            " And even though I know these tears come with a pain\r\n",
            " Even and just the same\r\n",
            "\r\n",
            " Chorus x2\r\n",
            "\r\n",
            " Verse 3\r\n",
            " The seas are full of water\r\n",
            " Stops by my shore\r\n",
            " All over the track like a feature\r\n",
            " And never wants to\r\n",
            " So tell me when it kicks in\r\n",
            "\r\n",
            " Pre Chorus\r\n",
            " People fall in love with you every single day\r\n",
            " And I just wanna tell you I am\r\n",
            "\r\n",
            " Chorus\r\n",
            " So now \r\n",
            "\r\n",
            " we're just beyond\r\n",
            "\r\n",
            "\r\n",
            " I don't know when I lost my\r\n",
            " Maybe it was every\r\n",
            " You You You You You You You You You You You You You You You You You You You You You You You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplexity**"
      ],
      "metadata": {
        "id": "_Q6NmWWZOao9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_perplexity(model, data_loader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Computes perplexity for a given trained model on a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained language model (LyricRNN or LyricGRU)\n",
        "    - data_loader: DataLoader containing the test data\n",
        "    - device: 'cpu' or 'cuda'\n",
        "\n",
        "    Returns:\n",
        "    - perplexity: Perplexity score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_words = 0\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')  # Sum loss across words\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_loader:\n",
        "            X, y = X.to(device), y.to(device)  # Move data to device\n",
        "\n",
        "            logits = model(X)  # (batch, seq_len, vocab_size)\n",
        "            vocab_size = logits.shape[-1]\n",
        "\n",
        "            # Reshape for loss calculation: (batch*seq_len, vocab_size)\n",
        "            logits = logits.view(-1, vocab_size)\n",
        "            y = y.view(-1)  # Flatten targets: (batch*seq_len)\n",
        "\n",
        "            loss = loss_fn(logits, y)\n",
        "            total_loss += loss.item()\n",
        "            total_words += y.numel()\n",
        "\n",
        "    avg_loss = total_loss / total_words\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "    return perplexity.item()\n"
      ],
      "metadata": {
        "id": "kubbyEJMynUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_lyrics(model_type, df, artist, prompt, num_to_generate=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    dataset, train_loader, trained_model = train_model(model_type, df, artist)\n",
        "    generated_text = generate_text_stochastic(trained_model, dataset, prompt, num_to_generate)\n",
        "\n",
        "    print(f\"\\nGenerated Text for {artist}:\\n\")\n",
        "    print(generated_text)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    train_perplexity = compute_perplexity(trained_model, train_loader)\n",
        "    print(f\"Perplexity for {artist} using {model_type}: {train_perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "BDyAA6tBUF1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_lyrics(\"RNN\", df, \"Ed Sheeran\", \"We found love\")"
      ],
      "metadata": {
        "id": "rZV31_EuZnUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bc9c80-9756-4606-fbce-df0c2c4306be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN for Ed Sheeran..\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.1745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 41.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Loss: 0.1819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:32<00:00, 41.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Loss: 0.1581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:32<00:00, 41.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Loss: 0.1470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Loss: 0.1411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Loss: 0.1370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Loss: 0.1339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Loss: 0.1318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 41.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Loss: 0.1296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Loss: 0.1281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 41.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Loss: 0.1265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Loss: 0.1254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:32<00:00, 41.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Loss: 0.1242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:33<00:00, 40.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Loss: 0.1241\n",
            "Model converged after 14 epochs.\n",
            "\n",
            "Generated Text for Ed Sheeran:\n",
            "\n",
            "We found love right\n",
            "\n",
            " And oh I've known it for the longest time\n",
            " And all my hope\n",
            " All my words are all over written on the signs\n",
            " But you're on my road walking me home\n",
            " home\n",
            "\n",
            " See the flames inside my eyes\n",
            " It burns so bright I wanna feel your love\n",
            " Easy baby maybe I'm a liar\n",
            " But for tonight I wanna fall in love\n",
            " And put your faith in my stomach\n",
            "\n",
            " I messed up this time\n",
            " Late last night\n",
            " Drinking to suppress devotion\n",
            " With fingers intertwined\n",
            " I can't shake this feeling now\n",
            " We're going through the motions\n",
            " Hoping you'd stop\n",
            "\n",
            " And oh I've only caused you pain\n",
            " You know but all of my words will always below\n",
            " Of all the love you spoke\n",
            " When you're on my road walking me home\n",
            " home\n",
            "\n",
            " See the flames inside my eyes\n",
            " It burns so bright I wanna feel your love\n",
            " Easy baby maybe I'm a liar\n",
            " But for tonight I wanna fall in love\n",
            " You see your\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_lyrics(\"GRU\", df, \"Ed Sheeran\", \"We found love\")"
      ],
      "metadata": {
        "id": "gUxWVW8FP3je",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "5554d7b6-369e-4605-9e67-58c863ad3e29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training GRU for Ed Sheeran..\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1365/1365 [00:35<00:00, 38.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.4373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 462/1365 [00:12<00:24, 37.32it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-af7bdff25f5a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_lyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GRU\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ed Sheeran\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"We found love\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-d39c1363c2ea>\u001b[0m in \u001b[0;36mwrite_lyrics\u001b[0;34m(model_type, df, artist, prompt, num_to_generate, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_lyrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_to_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text_stochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_to_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nGenerated Text for {artist}:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-200a1408c51a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_type, df, artist, num_epochs, batch_size, lr, convergence_threshold, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e9e3b55035d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (batch, seq_len, embed_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (batch, seq_len, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_lyrics(\"LSTM\", df, \"Ed Sheeran\", \"We found love\")"
      ],
      "metadata": {
        "id": "6hQSsPQodH7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}